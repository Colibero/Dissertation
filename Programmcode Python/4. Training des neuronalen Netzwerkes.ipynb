{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be870514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hulapalu\n",
      "We're using => cuda\n",
      "The data lies here => D:/Christof/cnn_database_tests/cd206/\n",
      "Size Dataset: 14104\n",
      "2820\n",
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485d792de2e94d92a4b17ba357b11f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=149.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new learning Rate: 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: | Train Loss: 0.46180 | Val Loss: 0.31701 |         Train Acc: 83.390| Val Acc: 90.000\n",
      "1\n",
      "Global minimal validation loss:  69\n",
      "new learning Rate: 5e-06\n",
      "Epoch 02: | Train Loss: 0.43131 | Val Loss: 0.24605 |         Train Acc: 84.028| Val Acc: 95.000\n",
      "2\n",
      "Global minimal validation loss:  69\n",
      "new learning Rate: 5e-06\n",
      "Epoch 03: | Train Loss: 0.42761 | Val Loss: 0.54000 |         Train Acc: 84.000| Val Acc: 80.000\n",
      "3\n",
      "Global minimal validation loss:  69\n",
      "new learning Rate: 5e-06\n",
      "Epoch 04: | Train Loss: 0.42240 | Val Loss: 0.50965 |         Train Acc: 83.972| Val Acc: 80.000\n",
      "4\n",
      "Global minimal validation loss:  69\n",
      "new learning Rate: 5e-06\n",
      "Epoch 05: | Train Loss: 0.41877 | Val Loss: 0.39506 |         Train Acc: 83.994| Val Acc: 85.000\n",
      "5\n",
      "Global minimal validation loss:  69\n",
      "new learning Rate: 5e-06\n",
      "Epoch 06: | Train Loss: 0.41328 | Val Loss: 0.28290 |         Train Acc: 84.085| Val Acc: 95.000\n",
      "6\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 07: | Train Loss: 0.41072 | Val Loss: 0.24992 |         Train Acc: 84.090| Val Acc: 95.000\n",
      "7\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 08: | Train Loss: 0.40711 | Val Loss: 0.34883 |         Train Acc: 84.085| Val Acc: 90.000\n",
      "8\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 09: | Train Loss: 0.39784 | Val Loss: 0.58551 |         Train Acc: 84.215| Val Acc: 70.000\n",
      "9\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 10: | Train Loss: 0.39602 | Val Loss: 0.48779 |         Train Acc: 84.277| Val Acc: 80.000\n",
      "10\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 11: | Train Loss: 0.38837 | Val Loss: 0.32716 |         Train Acc: 84.503| Val Acc: 90.000\n",
      "11\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 12: | Train Loss: 0.38176 | Val Loss: 0.36596 |         Train Acc: 84.576| Val Acc: 90.000\n",
      "12\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 13: | Train Loss: 0.37668 | Val Loss: 0.32136 |         Train Acc: 84.763| Val Acc: 85.000\n",
      "13\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 14: | Train Loss: 0.37042 | Val Loss: 0.31513 |         Train Acc: 84.864| Val Acc: 85.000\n",
      "14\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 15: | Train Loss: 0.36701 | Val Loss: 0.25692 |         Train Acc: 84.910| Val Acc: 90.000\n",
      "15\n",
      "Global minimal validation loss:  0.2460480034351349\n",
      "new learning Rate: 5e-06\n",
      "Epoch 16: | Train Loss: 0.36047 | Val Loss: 0.17293 |         Train Acc: 85.339| Val Acc: 95.000\n",
      "16\n",
      "Global minimal validation loss:  0.17293253540992737\n",
      "new learning Rate: 5e-06\n",
      "Epoch 17: | Train Loss: 0.35476 | Val Loss: 0.31524 |         Train Acc: 85.520| Val Acc: 90.000\n",
      "17\n",
      "Global minimal validation loss:  0.17293253540992737\n",
      "new learning Rate: 5e-06\n",
      "Epoch 18: | Train Loss: 0.35424 | Val Loss: 0.15472 |         Train Acc: 85.706| Val Acc: 95.000\n",
      "18\n",
      "Global minimal validation loss:  0.15471702814102173\n",
      "new learning Rate: 5e-06\n",
      "Epoch 19: | Train Loss: 0.35205 | Val Loss: 0.15052 |         Train Acc: 85.503| Val Acc: 100.000\n",
      "19\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 20: | Train Loss: 0.34517 | Val Loss: 0.40015 |         Train Acc: 85.881| Val Acc: 90.000\n",
      "20\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 21: | Train Loss: 0.34423 | Val Loss: 0.25189 |         Train Acc: 85.740| Val Acc: 90.000\n",
      "21\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 22: | Train Loss: 0.34197 | Val Loss: 0.30223 |         Train Acc: 86.079| Val Acc: 85.000\n",
      "22\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 23: | Train Loss: 0.33580 | Val Loss: 0.23281 |         Train Acc: 86.023| Val Acc: 90.000\n",
      "23\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 24: | Train Loss: 0.33523 | Val Loss: 0.19732 |         Train Acc: 86.322| Val Acc: 95.000\n",
      "24\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 25: | Train Loss: 0.33095 | Val Loss: 0.23922 |         Train Acc: 86.492| Val Acc: 90.000\n",
      "25\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 26: | Train Loss: 0.33126 | Val Loss: 0.56191 |         Train Acc: 86.345| Val Acc: 80.000\n",
      "26\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 27: | Train Loss: 0.32013 | Val Loss: 0.16899 |         Train Acc: 86.780| Val Acc: 95.000\n",
      "27\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 28: | Train Loss: 0.32864 | Val Loss: 0.47384 |         Train Acc: 86.486| Val Acc: 70.000\n",
      "28\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 29: | Train Loss: 0.31730 | Val Loss: 0.34567 |         Train Acc: 86.921| Val Acc: 90.000\n",
      "29\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 30: | Train Loss: 0.31267 | Val Loss: 0.27227 |         Train Acc: 86.994| Val Acc: 95.000\n",
      "30\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 31: | Train Loss: 0.31527 | Val Loss: 0.48105 |         Train Acc: 86.661| Val Acc: 80.000\n",
      "31\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 32: | Train Loss: 0.31312 | Val Loss: 0.23937 |         Train Acc: 86.859| Val Acc: 85.000\n",
      "32\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 33: | Train Loss: 0.30984 | Val Loss: 0.50670 |         Train Acc: 86.994| Val Acc: 80.000\n",
      "33\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 34: | Train Loss: 0.31003 | Val Loss: 0.39769 |         Train Acc: 86.932| Val Acc: 80.000\n",
      "34\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 35: | Train Loss: 0.30961 | Val Loss: 0.37125 |         Train Acc: 86.904| Val Acc: 90.000\n",
      "35\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 36: | Train Loss: 0.30594 | Val Loss: 0.23912 |         Train Acc: 87.260| Val Acc: 80.000\n",
      "36\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 37: | Train Loss: 0.30048 | Val Loss: 0.29325 |         Train Acc: 87.655| Val Acc: 95.000\n",
      "37\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 38: | Train Loss: 0.29969 | Val Loss: 0.38352 |         Train Acc: 87.565| Val Acc: 75.000\n",
      "38\n",
      "Global minimal validation loss:  0.15052425861358643\n",
      "new learning Rate: 5e-06\n",
      "Epoch 39: | Train Loss: 0.29836 | Val Loss: 0.15020 |         Train Acc: 87.576| Val Acc: 100.000\n",
      "39\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 40: | Train Loss: 0.29860 | Val Loss: 0.29755 |         Train Acc: 87.469| Val Acc: 95.000\n",
      "40\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 41: | Train Loss: 0.29702 | Val Loss: 0.17130 |         Train Acc: 87.333| Val Acc: 95.000\n",
      "41\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 42: | Train Loss: 0.29107 | Val Loss: 0.19597 |         Train Acc: 87.814| Val Acc: 90.000\n",
      "42\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 43: | Train Loss: 0.29073 | Val Loss: 0.47557 |         Train Acc: 87.582| Val Acc: 85.000\n",
      "43\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 44: | Train Loss: 0.29281 | Val Loss: 0.49480 |         Train Acc: 87.463| Val Acc: 85.000\n",
      "44\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 45: | Train Loss: 0.28946 | Val Loss: 0.18021 |         Train Acc: 87.853| Val Acc: 95.000\n",
      "45\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 46: | Train Loss: 0.28816 | Val Loss: 0.17149 |         Train Acc: 88.006| Val Acc: 95.000\n",
      "46\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 47: | Train Loss: 0.28153 | Val Loss: 0.15573 |         Train Acc: 88.282| Val Acc: 95.000\n",
      "47\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: | Train Loss: 0.28504 | Val Loss: 0.24820 |         Train Acc: 87.944| Val Acc: 85.000\n",
      "48\n",
      "Global minimal validation loss:  0.1501951664686203\n",
      "new learning Rate: 5e-06\n",
      "Epoch 49: | Train Loss: 0.27953 | Val Loss: 0.14818 |         Train Acc: 88.203| Val Acc: 90.000\n",
      "49\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 50: | Train Loss: 0.28274 | Val Loss: 0.16761 |         Train Acc: 87.881| Val Acc: 90.000\n",
      "50\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 51: | Train Loss: 0.27961 | Val Loss: 0.16813 |         Train Acc: 87.938| Val Acc: 90.000\n",
      "51\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 52: | Train Loss: 0.27587 | Val Loss: 0.23117 |         Train Acc: 88.266| Val Acc: 80.000\n",
      "52\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 53: | Train Loss: 0.28534 | Val Loss: 0.59082 |         Train Acc: 88.153| Val Acc: 70.000\n",
      "53\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 54: | Train Loss: 0.27287 | Val Loss: 0.37247 |         Train Acc: 88.650| Val Acc: 95.000\n",
      "54\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 55: | Train Loss: 0.27253 | Val Loss: 0.22510 |         Train Acc: 88.169| Val Acc: 90.000\n",
      "55\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 56: | Train Loss: 0.27424 | Val Loss: 0.42730 |         Train Acc: 88.220| Val Acc: 80.000\n",
      "56\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 57: | Train Loss: 0.27121 | Val Loss: 0.25608 |         Train Acc: 88.599| Val Acc: 90.000\n",
      "57\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 58: | Train Loss: 0.27140 | Val Loss: 0.24848 |         Train Acc: 88.616| Val Acc: 85.000\n",
      "58\n",
      "Global minimal validation loss:  0.14818328619003296\n",
      "new learning Rate: 5e-06\n",
      "Epoch 59: | Train Loss: 0.27072 | Val Loss: 0.12252 |         Train Acc: 88.565| Val Acc: 100.000\n",
      "59\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 60: | Train Loss: 0.27172 | Val Loss: 0.15065 |         Train Acc: 88.395| Val Acc: 95.000\n",
      "60\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 61: | Train Loss: 0.26737 | Val Loss: 0.53288 |         Train Acc: 88.169| Val Acc: 65.000\n",
      "61\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 62: | Train Loss: 0.26477 | Val Loss: 0.25365 |         Train Acc: 88.774| Val Acc: 95.000\n",
      "62\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 63: | Train Loss: 0.26061 | Val Loss: 0.17595 |         Train Acc: 88.791| Val Acc: 95.000\n",
      "63\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 64: | Train Loss: 0.25857 | Val Loss: 0.62462 |         Train Acc: 88.831| Val Acc: 80.000\n",
      "64\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 65: | Train Loss: 0.26039 | Val Loss: 0.14109 |         Train Acc: 89.017| Val Acc: 95.000\n",
      "65\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 66: | Train Loss: 0.26049 | Val Loss: 0.17987 |         Train Acc: 89.017| Val Acc: 95.000\n",
      "66\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 67: | Train Loss: 0.25656 | Val Loss: 0.26558 |         Train Acc: 89.102| Val Acc: 85.000\n",
      "67\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 68: | Train Loss: 0.25133 | Val Loss: 0.20920 |         Train Acc: 89.328| Val Acc: 90.000\n",
      "68\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 69: | Train Loss: 0.25460 | Val Loss: 0.49281 |         Train Acc: 89.373| Val Acc: 65.000\n",
      "69\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 70: | Train Loss: 0.24789 | Val Loss: 0.21470 |         Train Acc: 89.452| Val Acc: 90.000\n",
      "70\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 71: | Train Loss: 0.24978 | Val Loss: 0.14523 |         Train Acc: 89.435| Val Acc: 95.000\n",
      "71\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 72: | Train Loss: 0.24821 | Val Loss: 0.51277 |         Train Acc: 89.525| Val Acc: 70.000\n",
      "72\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 73: | Train Loss: 0.24655 | Val Loss: 0.22324 |         Train Acc: 89.469| Val Acc: 85.000\n",
      "73\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 74: | Train Loss: 0.24543 | Val Loss: 0.47330 |         Train Acc: 89.661| Val Acc: 90.000\n",
      "74\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 75: | Train Loss: 0.24336 | Val Loss: 0.21384 |         Train Acc: 89.480| Val Acc: 90.000\n",
      "75\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 76: | Train Loss: 0.24213 | Val Loss: 0.19383 |         Train Acc: 89.288| Val Acc: 90.000\n",
      "76\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 77: | Train Loss: 0.23988 | Val Loss: 0.21329 |         Train Acc: 89.751| Val Acc: 85.000\n",
      "77\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 78: | Train Loss: 0.24103 | Val Loss: 0.13434 |         Train Acc: 90.090| Val Acc: 100.000\n",
      "78\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 79: | Train Loss: 0.23895 | Val Loss: 0.24714 |         Train Acc: 89.842| Val Acc: 90.000\n",
      "79\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 80: | Train Loss: 0.24059 | Val Loss: 0.18523 |         Train Acc: 90.130| Val Acc: 95.000\n",
      "80\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 81: | Train Loss: 0.24582 | Val Loss: 0.60936 |         Train Acc: 89.695| Val Acc: 80.000\n",
      "81\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 82: | Train Loss: 0.23429 | Val Loss: 0.39309 |         Train Acc: 90.090| Val Acc: 85.000\n",
      "82\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 83: | Train Loss: 0.23391 | Val Loss: 0.21864 |         Train Acc: 90.147| Val Acc: 90.000\n",
      "83\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 84: | Train Loss: 0.23264 | Val Loss: 0.28187 |         Train Acc: 90.079| Val Acc: 85.000\n",
      "84\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 85: | Train Loss: 0.23463 | Val Loss: 0.43368 |         Train Acc: 90.051| Val Acc: 80.000\n",
      "85\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 86: | Train Loss: 0.22991 | Val Loss: 0.39556 |         Train Acc: 90.181| Val Acc: 75.000\n",
      "86\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 87: | Train Loss: 0.22920 | Val Loss: 0.35266 |         Train Acc: 90.237| Val Acc: 75.000\n",
      "87\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 88: | Train Loss: 0.22571 | Val Loss: 0.53632 |         Train Acc: 90.418| Val Acc: 75.000\n",
      "88\n",
      "Global minimal validation loss:  0.12252436578273773\n",
      "new learning Rate: 5e-06\n",
      "Epoch 89: | Train Loss: 0.21912 | Val Loss: 0.12042 |         Train Acc: 90.842| Val Acc: 95.000\n",
      "89\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 90: | Train Loss: 0.22408 | Val Loss: 0.28055 |         Train Acc: 90.542| Val Acc: 90.000\n",
      "90\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 91: | Train Loss: 0.22129 | Val Loss: 0.38522 |         Train Acc: 90.729| Val Acc: 85.000\n",
      "91\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 92: | Train Loss: 0.21446 | Val Loss: 0.18060 |         Train Acc: 90.836| Val Acc: 95.000\n",
      "92\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 93: | Train Loss: 0.22236 | Val Loss: 0.68092 |         Train Acc: 90.814| Val Acc: 70.000\n",
      "93\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 94: | Train Loss: 0.21928 | Val Loss: 0.20545 |         Train Acc: 90.831| Val Acc: 90.000\n",
      "94\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: | Train Loss: 0.21076 | Val Loss: 0.17511 |         Train Acc: 91.226| Val Acc: 90.000\n",
      "95\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 96: | Train Loss: 0.21694 | Val Loss: 0.18017 |         Train Acc: 90.927| Val Acc: 90.000\n",
      "96\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 97: | Train Loss: 0.21103 | Val Loss: 0.18139 |         Train Acc: 91.288| Val Acc: 90.000\n",
      "97\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 98: | Train Loss: 0.21107 | Val Loss: 0.15443 |         Train Acc: 91.395| Val Acc: 95.000\n",
      "98\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 99: | Train Loss: 0.21468 | Val Loss: 0.21919 |         Train Acc: 90.893| Val Acc: 90.000\n",
      "99\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 100: | Train Loss: 0.20669 | Val Loss: 0.34852 |         Train Acc: 91.508| Val Acc: 90.000\n",
      "100\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 101: | Train Loss: 0.20763 | Val Loss: 0.64822 |         Train Acc: 91.345| Val Acc: 75.000\n",
      "101\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 102: | Train Loss: 0.20417 | Val Loss: 0.36069 |         Train Acc: 91.605| Val Acc: 85.000\n",
      "102\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 103: | Train Loss: 0.20105 | Val Loss: 0.15850 |         Train Acc: 91.757| Val Acc: 95.000\n",
      "103\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 104: | Train Loss: 0.20368 | Val Loss: 0.19919 |         Train Acc: 91.525| Val Acc: 90.000\n",
      "104\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 105: | Train Loss: 0.19611 | Val Loss: 0.17556 |         Train Acc: 91.712| Val Acc: 95.000\n",
      "105\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 106: | Train Loss: 0.19286 | Val Loss: 0.45986 |         Train Acc: 92.056| Val Acc: 80.000\n",
      "106\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 107: | Train Loss: 0.19947 | Val Loss: 0.33473 |         Train Acc: 91.655| Val Acc: 85.000\n",
      "107\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 108: | Train Loss: 0.18882 | Val Loss: 0.15129 |         Train Acc: 92.288| Val Acc: 95.000\n",
      "108\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 109: | Train Loss: 0.19383 | Val Loss: 0.16977 |         Train Acc: 91.932| Val Acc: 95.000\n",
      "109\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 110: | Train Loss: 0.19413 | Val Loss: 0.28062 |         Train Acc: 91.910| Val Acc: 90.000\n",
      "110\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 111: | Train Loss: 0.19776 | Val Loss: 0.21619 |         Train Acc: 91.972| Val Acc: 90.000\n",
      "111\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 112: | Train Loss: 0.18923 | Val Loss: 0.21570 |         Train Acc: 92.215| Val Acc: 95.000\n",
      "112\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 113: | Train Loss: 0.18593 | Val Loss: 0.13211 |         Train Acc: 92.294| Val Acc: 95.000\n",
      "113\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 114: | Train Loss: 0.18214 | Val Loss: 0.18179 |         Train Acc: 92.384| Val Acc: 90.000\n",
      "114\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 115: | Train Loss: 0.18509 | Val Loss: 0.34959 |         Train Acc: 92.243| Val Acc: 90.000\n",
      "115\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 116: | Train Loss: 0.18752 | Val Loss: 0.18195 |         Train Acc: 92.220| Val Acc: 90.000\n",
      "116\n",
      "Global minimal validation loss:  0.12042325735092163\n",
      "new learning Rate: 5e-06\n",
      "Epoch 117: | Train Loss: 0.18295 | Val Loss: 0.06076 |         Train Acc: 92.384| Val Acc: 100.000\n",
      "117\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 118: | Train Loss: 0.18282 | Val Loss: 0.10390 |         Train Acc: 92.463| Val Acc: 100.000\n",
      "118\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 119: | Train Loss: 0.18047 | Val Loss: 0.20790 |         Train Acc: 92.497| Val Acc: 90.000\n",
      "119\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 120: | Train Loss: 0.17604 | Val Loss: 0.25343 |         Train Acc: 92.915| Val Acc: 90.000\n",
      "120\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 121: | Train Loss: 0.17918 | Val Loss: 0.34052 |         Train Acc: 92.655| Val Acc: 85.000\n",
      "121\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 122: | Train Loss: 0.18228 | Val Loss: 0.51111 |         Train Acc: 92.542| Val Acc: 90.000\n",
      "122\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 123: | Train Loss: 0.17125 | Val Loss: 0.40389 |         Train Acc: 93.209| Val Acc: 85.000\n",
      "123\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 124: | Train Loss: 0.17238 | Val Loss: 0.26151 |         Train Acc: 92.853| Val Acc: 90.000\n",
      "124\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 125: | Train Loss: 0.16618 | Val Loss: 0.17191 |         Train Acc: 93.260| Val Acc: 90.000\n",
      "125\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 126: | Train Loss: 0.16286 | Val Loss: 0.51576 |         Train Acc: 93.446| Val Acc: 80.000\n",
      "126\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 127: | Train Loss: 0.16045 | Val Loss: 0.14036 |         Train Acc: 93.565| Val Acc: 95.000\n",
      "127\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 128: | Train Loss: 0.16070 | Val Loss: 0.08384 |         Train Acc: 93.435| Val Acc: 95.000\n",
      "128\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 129: | Train Loss: 0.15818 | Val Loss: 0.12354 |         Train Acc: 93.514| Val Acc: 95.000\n",
      "129\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 130: | Train Loss: 0.16518 | Val Loss: 0.20359 |         Train Acc: 93.102| Val Acc: 90.000\n",
      "130\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 131: | Train Loss: 0.15867 | Val Loss: 0.21609 |         Train Acc: 93.503| Val Acc: 95.000\n",
      "131\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 132: | Train Loss: 0.16162 | Val Loss: 0.07738 |         Train Acc: 93.395| Val Acc: 100.000\n",
      "132\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 133: | Train Loss: 0.15982 | Val Loss: 0.51557 |         Train Acc: 93.531| Val Acc: 85.000\n",
      "133\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 134: | Train Loss: 0.15569 | Val Loss: 0.17416 |         Train Acc: 93.356| Val Acc: 85.000\n",
      "134\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 135: | Train Loss: 0.14449 | Val Loss: 0.14512 |         Train Acc: 94.181| Val Acc: 95.000\n",
      "135\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 136: | Train Loss: 0.14620 | Val Loss: 0.31289 |         Train Acc: 93.847| Val Acc: 75.000\n",
      "136\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 137: | Train Loss: 0.14642 | Val Loss: 0.09969 |         Train Acc: 94.186| Val Acc: 95.000\n",
      "137\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 138: | Train Loss: 0.14640 | Val Loss: 0.07438 |         Train Acc: 94.147| Val Acc: 95.000\n",
      "138\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 139: | Train Loss: 0.14124 | Val Loss: 0.19974 |         Train Acc: 94.277| Val Acc: 90.000\n",
      "139\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 140: | Train Loss: 0.13828 | Val Loss: 0.19748 |         Train Acc: 94.345| Val Acc: 90.000\n",
      "140\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141: | Train Loss: 0.14819 | Val Loss: 0.16857 |         Train Acc: 93.955| Val Acc: 90.000\n",
      "141\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 142: | Train Loss: 0.14491 | Val Loss: 0.10041 |         Train Acc: 94.113| Val Acc: 95.000\n",
      "142\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 143: | Train Loss: 0.14040 | Val Loss: 0.12031 |         Train Acc: 94.401| Val Acc: 100.000\n",
      "143\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 144: | Train Loss: 0.13730 | Val Loss: 0.07878 |         Train Acc: 94.508| Val Acc: 100.000\n",
      "144\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 145: | Train Loss: 0.14014 | Val Loss: 0.61355 |         Train Acc: 94.571| Val Acc: 80.000\n",
      "145\n",
      "Global minimal validation loss:  0.060764819383621216\n",
      "new learning Rate: 5e-06\n",
      "Epoch 146: | Train Loss: 0.12965 | Val Loss: 0.05272 |         Train Acc: 94.831| Val Acc: 100.000\n",
      "146\n",
      "Global minimal validation loss:  0.052724532783031464\n",
      "new learning Rate: 5e-06\n",
      "Epoch 147: | Train Loss: 0.13674 | Val Loss: 0.17500 |         Train Acc: 94.288| Val Acc: 95.000\n",
      "147\n",
      "Global minimal validation loss:  0.052724532783031464\n",
      "new learning Rate: 5e-06\n",
      "Epoch 148: | Train Loss: 0.13577 | Val Loss: 0.09992 |         Train Acc: 94.610| Val Acc: 100.000\n",
      "148\n",
      "Global minimal validation loss:  0.052724532783031464\n",
      "new learning Rate: 5e-06\n",
      "Epoch 149: | Train Loss: 0.13186 | Val Loss: 0.22704 |         Train Acc: 94.751| Val Acc: 90.000\n",
      "149\n",
      "Global minimal validation loss:  0.052724532783031464\n",
      "\n",
      "{1: 0.3170086741447449, 2: 0.2460480034351349, 3: 0.5399957299232483, 4: 0.50965416431427, 5: 0.39505523443222046, 6: 0.28290119767189026, 7: 0.2499237060546875, 8: 0.3488260507583618, 9: 0.5855065584182739, 10: 0.4877933859825134, 11: 0.32715752720832825, 12: 0.3659648299217224, 13: 0.32135578989982605, 14: 0.3151342272758484, 15: 0.25691577792167664, 16: 0.17293253540992737, 17: 0.31524401903152466, 18: 0.15471702814102173, 19: 0.15052425861358643, 20: 0.40015000104904175, 21: 0.2518850266933441, 22: 0.3022271692752838, 23: 0.2328086644411087, 24: 0.19732114672660828, 25: 0.23922200500965118, 26: 0.5619112253189087, 27: 0.16898545622825623, 28: 0.47383904457092285, 29: 0.3456677794456482, 30: 0.2722682058811188, 31: 0.48105281591415405, 32: 0.23937466740608215, 33: 0.5067034959793091, 34: 0.3976934552192688, 35: 0.3712517321109772, 36: 0.23911578953266144, 37: 0.2932489514350891, 38: 0.3835189640522003, 39: 0.1501951664686203, 40: 0.29755130410194397, 41: 0.17129656672477722, 42: 0.195968359708786, 43: 0.47556909918785095, 44: 0.4947952628135681, 45: 0.1802121251821518, 46: 0.17149470746517181, 47: 0.15572522580623627, 48: 0.24820241332054138, 49: 0.14818328619003296, 50: 0.16760842502117157, 51: 0.16812889277935028, 52: 0.23117272555828094, 53: 0.5908201932907104, 54: 0.37247100472450256, 55: 0.2250957041978836, 56: 0.42730122804641724, 57: 0.2560836672782898, 58: 0.24848265945911407, 59: 0.12252436578273773, 60: 0.15064577758312225, 61: 0.5328766703605652, 62: 0.25365450978279114, 63: 0.17594747245311737, 64: 0.6246176958084106, 65: 0.14108550548553467, 66: 0.17986822128295898, 67: 0.26557567715644836, 68: 0.2091968059539795, 69: 0.4928087592124939, 70: 0.21469929814338684, 71: 0.14522865414619446, 72: 0.5127748250961304, 73: 0.2232380211353302, 74: 0.47330379486083984, 75: 0.21383920311927795, 76: 0.19382517039775848, 77: 0.21328642964363098, 78: 0.1343383938074112, 79: 0.24714262783527374, 80: 0.18523184955120087, 81: 0.6093586683273315, 82: 0.39309436082839966, 83: 0.21864083409309387, 84: 0.2818748652935028, 85: 0.4336835741996765, 86: 0.3955593407154083, 87: 0.35266464948654175, 88: 0.5363187193870544, 89: 0.12042325735092163, 90: 0.2805507481098175, 91: 0.3852165639400482, 92: 0.18059608340263367, 93: 0.6809176206588745, 94: 0.2054508626461029, 95: 0.17511209845542908, 96: 0.1801704466342926, 97: 0.18139097094535828, 98: 0.15443187952041626, 99: 0.2191910445690155, 100: 0.34852296113967896, 101: 0.6482200026512146, 102: 0.36069390177726746, 103: 0.1584966480731964, 104: 0.19918832182884216, 105: 0.1755603849887848, 106: 0.45985665917396545, 107: 0.33472752571105957, 108: 0.151286318898201, 109: 0.16977062821388245, 110: 0.2806236147880554, 111: 0.21619346737861633, 112: 0.21570196747779846, 113: 0.13210907578468323, 114: 0.1817878931760788, 115: 0.3495868146419525, 116: 0.18195433914661407, 117: 0.060764819383621216, 118: 0.10389937460422516, 119: 0.20789821445941925, 120: 0.2534310817718506, 121: 0.3405225872993469, 122: 0.5111127495765686, 123: 0.4038906693458557, 124: 0.2615053057670593, 125: 0.17190872132778168, 126: 0.5157636404037476, 127: 0.14036458730697632, 128: 0.08384187519550323, 129: 0.12353827804327011, 130: 0.2035934031009674, 131: 0.21608524024486542, 132: 0.07738368213176727, 133: 0.5155671238899231, 134: 0.1741618812084198, 135: 0.14512252807617188, 136: 0.3128892183303833, 137: 0.0996859222650528, 138: 0.07438312470912933, 139: 0.19973750412464142, 140: 0.19747516512870789, 141: 0.16856878995895386, 142: 0.10040770471096039, 143: 0.1203087568283081, 144: 0.07877713441848755, 145: 0.6135465502738953, 146: 0.052724532783031464, 147: 0.17500045895576477, 148: 0.09992068260908127, 149: 0.2270403802394867}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-00939cd2c0dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m \u001b[0mtrain_val_acc_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[0mtrain_val_loss_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Code for the training of a neuronal network based on two folders with images with one containing tiles with \n",
    "a positive cell and the other containing images of negative tiles\n",
    "\"\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"We're using =>\", device)\n",
    "\n",
    "#location of the root directory\n",
    "root_dir = \"D:/Christof/cnn_database_tests/cd206/\"\n",
    "print(\"The data lies here =>\", root_dir)\n",
    "\n",
    "#definition of the image transformation function prior to training \n",
    "image_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((70, 70)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(), \n",
    "        \n",
    "    ]),\n",
    "    \"validation\": transforms.Compose([\n",
    "        transforms.Resize((70, 70)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    }\n",
    "\n",
    "#location of the dataset with the training data       \n",
    "dataset = datasets.ImageFolder(root = root_dir + \"train\",\n",
    "                              transform = image_transforms[\"train\"]\n",
    "                             )\n",
    "        \n",
    "dataset\n",
    "\n",
    "#allocation of the positive and negative regions to the corresponding classes for training\n",
    "dataset.class_to_idx\n",
    "\n",
    "idx2class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}    \n",
    "    \n",
    "    for _, label_id in dataset_obj:\n",
    "        label = idx2class[label_id]\n",
    "        count_dict[label] += 1\n",
    "    return count_dict\n",
    "\n",
    "#description of the dataset\n",
    "dataset_size = len(dataset)\n",
    "print('Size Dataset:' , dataset_size)\n",
    "dataset_indices = list(range(dataset_size))\n",
    "\n",
    "np.random.shuffle(dataset_indices)\n",
    "\n",
    "#splitting of the dataset in training, validation and test set set\n",
    "val_split_index = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "print(val_split_index)\n",
    "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "dataset_test = datasets.ImageFolder(root = root_dir + \"test\",\n",
    "                                            transform = image_transforms[\"validation\"]\n",
    "                                           )\n",
    "dataset_test\n",
    "\n",
    "#Dataloader for the Datasets for training, validation and testing\n",
    "train_loader = DataLoader(dataset= dataset, shuffle=False, batch_size=64, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset= dataset, shuffle=False, batch_size=1, sampler=val_sampler)\n",
    "test_loader = DataLoader(dataset= dataset_test, shuffle=False, batch_size=1)\n",
    "\n",
    "def get_class_distribution_loaders(dataloader_obj, dataset_obj):\n",
    "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}    \n",
    "    \n",
    "    if dataloader_obj.batch_size == 1:    \n",
    "        for _,label_id in dataloader_obj:\n",
    "            y_idx = label_id.item()\n",
    "            y_lbl = idx2class[y_idx]\n",
    "            count_dict[str(y_lbl)] += 1\n",
    "    else: \n",
    "        for _,label_id in dataloader_obj:\n",
    "            for idx in label_id:\n",
    "                y_idx = idx.item()\n",
    "                y_lbl = idx2class[y_idx]\n",
    "                count_dict[str(y_lbl)] += 1\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "single_batch = next(iter(train_loader))\n",
    "\n",
    "single_batch[0].shape\n",
    "\n",
    "\n",
    "#definition of the model, loading a preexisting resnet18 and sending it to device\n",
    "model =  torchvision.models.resnet18(pretrained = False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.to(device)\n",
    "\n",
    "#setting the learning rate\n",
    "learning_rate = 0.000005\n",
    "#definition of the loss criterion and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "#definition of the metrics used for the validation of the trained net\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)    \n",
    "    correct_results_sum = (y_pred_tags == y_test).sum().float()    \n",
    "    \n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)    \n",
    "    \n",
    "    return acc\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "print(\"Begin training.\")\n",
    "\n",
    "#Training and Validation of the net\n",
    "\n",
    "#e: numboer of training iterations\n",
    "for e in tqdm(range(1, 50)):    \n",
    "    print('learning Rate:' , learning_rate)\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0    \n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "        y_train_pred = model(X_train_batch).squeeze()        \n",
    "\n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = binary_acc(y_train_pred, y_train_batch)        \n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "    # VALIDATION\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)            \n",
    "\n",
    "            y_val_pred = model(X_val_batch).squeeze()\n",
    "            y_val_pred = torch.unsqueeze(y_val_pred, 0)            \n",
    "\n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = binary_acc(y_val_pred, y_val_batch)            \n",
    "\n",
    "            val_epoch_loss += train_loss.item()\n",
    "            val_epoch_acc += train_acc.item()    \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "    print(f'Epoch {e+0:02}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | \\\n",
    "    Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "    \n",
    "#save the trained model    \n",
    "torch.save(model.state_dict(), 'D:/Christof/trained_networks/211001_resnet50_trained_cd206.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'D:/Christof/trained_networks/211001_resnet50_trained_cd206.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224cce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "lists = sorted(dict_loss.items()) # sorted by key, return a list of tuples\n",
    "\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "plt.yscale('log')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa173cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-048a2c07b1b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkey_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(dict_loss.get(min))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "key_min = min(dict_loss.keys(), key=(lambda k: dict_loss[k]))\n",
    "print(min)\n",
    "#print(dict_loss.get(min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c3e460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.997771330177784, 2.859957441687584, 3.8086727410554886, 4.7623143792152405, 5.697222501039505]\n",
      "3.82518767863512\n"
     ]
    }
   ],
   "source": [
    "loss_last_5 =[dict_loss.get(e), dict_loss.get(e - 1), dict_loss.get(e - 2), dict_loss.get(e - 3), dict_loss.get(e - 4) ]\n",
    "\n",
    "mean_last_5 = np.mean(loss_last_5)\n",
    "print(mean_last_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d280b716",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d29659d44474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_pred_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_true_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)        \n",
    "        \n",
    "        y_test_pred = model(x_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())\n",
    "        \n",
    "y_pred_list = [[i] for i in y_pred_list]\n",
    "\n",
    "y_true_list = [i[0] for i in y_true_list]\n",
    "\n",
    "print(classification_report(y_true_list, y_pred_list))\n",
    "\n",
    "print(confusion_matrix(y_true_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
